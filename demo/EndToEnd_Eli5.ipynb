{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSMARCO Model Performing on ELI5 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Agent Interaction\n",
    "\n",
    "## Contributions\n",
    "1. A system that takes in an input query and is capable of either generating a dialogue response or answering the question if possible given its existing knowledge, as well as identifying the intent of the input.\n",
    "2. Our finetuned models of various sizes and trained on various datasets that are capable of answering questions reasonably well from any context its given, and ones for intent identification under both classification and masked language modelling cases.\n",
    "3. An open-source codebase to be able to easily finetune any of the existing models (ours or huggingface's) on your own custom dataset. Useful for if you have domain-specific data.\n",
    "\n",
    "## End-to-End Pipeline\n",
    "1. Classify if the user query is something that requires a generic dialogue response or a question (Did they say \"hello\" or did they say \"how do I get my TCard?\"\n",
    "2. If generic dialogue, use the generic dialogue model to generate a response\n",
    "3. If a question, identify the intent of the question\n",
    "4. Use a SentenceTransformer to identify which context is most similar to the question (therefore most likely to contain the answer)\n",
    "5. Input the selected context and the user query into the question-answering model\n",
    "\n",
    "## In This Notebook\n",
    "- **Binary Classification**: Performed with intent identification model finetuned for binary classifaction\n",
    "- Dialogue: Performed by huggingface's pretrained DialoGPT\n",
    "- *Intent Identification*: (not present in this notebook)\n",
    "- Context Retrieval: Performed by a pretrained Sentence Transformer that identifies which context is the most semantically similar to the input query.\n",
    "- **Answer Generation**: Our finetuned GPT2 for QA\n",
    "\n",
    "## Future Steps\n",
    "- We can further improve the pipeline by finetuning the Dialogue and Context Retrieval steps.\n",
    "    - DialoGPT was trained on reddit data, and is prone to informal dialogue and Star Wars memes.\n",
    "    - SentenceTransformer can get finetuned on domain specific data to improve its accuracy for getting the best context. As things currently are, it is the biggest bottleneck affecting performance of the end-to-end agent if it provides a bad context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import (BertForMaskedLM, AutoTokenizer, AutoConfig, BertModel, BertConfig, \n",
    "BertTokenizer, BertForSequenceClassification, GPT2Tokenizer\n",
    ")\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util as sentenceutils\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, evaluation, losses, InputExample, datasets\n",
    "\n",
    "sys.path.insert(0,'../answer_generation')\n",
    "from utils import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LABEL_COLUMNS = ['general', 'request']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizers\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers.\n",
    "print('Loading tokenizers')\n",
    "berttokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "#gpt2tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier:\n",
    "    \"\"\"\n",
    "    Classify if it's general dialogue or a request.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        chk_path = PATH+\"/checkpoints/binary_classification/binary_classification.pth\"\n",
    "        checkpoint = torch.load(chk_path, map_location=torch.device('cpu'))\n",
    "\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2\n",
    "        )\n",
    "        self.model.load_state_dict(checkpoint)\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "        self.softmax_layer = torch.nn.Softmax()\n",
    "        self.LABEL_COLUMNS = ['general', 'request']\n",
    "    \n",
    "    def tokenize_sentences(self, sentences):\n",
    "        # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        # For every sentence...\n",
    "        for sent in sentences:\n",
    "            # encode_plus will:\n",
    "            #   (1) Tokenize the sentence.\n",
    "            #   (2) Prepend the [CLS] token to the start.\n",
    "            #   (3) Append the [SEP] token to the end.\n",
    "            #   (4) Map tokens to their IDs.\n",
    "            #   (5) Pad or truncate the sentence to max_length\n",
    "            #   (6) Create attention masks for [PAD] tokens.\n",
    "            encoded_dict = berttokenizer.encode_plus(\n",
    "                                sent,                      # Sentence to encode.\n",
    "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                max_length = 320,           # Pad & truncate all sentences.\n",
    "                                padding='max_length',\n",
    "                                return_attention_mask = True,   # Construct attn. masks.\n",
    "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        )\n",
    "\n",
    "            # Add the encoded sentence to the list.\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "            # And its attention mask (simply differentiates padding from non-padding).\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        return input_ids,attention_masks\n",
    "    \n",
    "    def classify(self, query, printout=True):\n",
    "        #tokenize inputted sentence to be compatible with BERT inputs\n",
    "        token_ids, attention_masks = self.tokenize_sentences([query])\n",
    "\n",
    "        #get a tensor containing probabilities of inputted sentence being irrelevant or relevant\n",
    "        model_outputs = (self.model(token_ids.to(device), token_type_ids=None, attention_mask=attention_masks.to(device)))\n",
    "        result = self.softmax_layer(model_outputs[0])\n",
    "\n",
    "        #identify which output node has higher probability and what that probability is\n",
    "        prediction = torch.argmax(result).item()\n",
    "        confidence = torch.max(result).item()\n",
    "        if printout:\n",
    "            print(\"The class is: \" + self.LABEL_COLUMNS[prediction] + \" with {:.2f}% confident\".format(confidence*100))\n",
    "\n",
    "        return LABEL_COLUMNS[prediction], confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueGeneration:\n",
    "    \"\"\"\n",
    "    Generic Dialogue Generation\n",
    "    \n",
    "    NOTE: DialoGPT is prone to making Star Wars references.\n",
    "    History for chat is not implemented\n",
    "    \"\"\"\n",
    "    def __init__(self, modelname=\"microsoft/DialoGPT-medium\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(modelname)\n",
    "        \n",
    "    def generate(self, query, printout=True):\n",
    "        # format input\n",
    "        step = 0\n",
    "        new_user_input_ids = self.tokenizer.encode(query + self.tokenizer.eos_token, return_tensors='pt')\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "        chat_history_ids = self.model.generate(bot_input_ids, max_length=1000, pad_token_id=self.tokenizer.eos_token_id)\n",
    "        \n",
    "        # generate response\n",
    "        response = self.tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "        \n",
    "        if printout:\n",
    "            print(\"DialoGPT Response:\", response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search & Re-Ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic search function performs the initial passage retrieval using a bi-encoder. The passage re-ranking is done using a cross encoder. Both are pre-trained encoders and implemented in the same function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/ssd003/projects/aieng/conversational_ai/demo/data/Eli5/Eli5_reranked/eli5_train_reranked.json', 'r') as f:\n",
    "    eli5 = json.load(f)\n",
    "eli5 = pd.read_json(eli5, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = []\n",
    "for i in range(0,len(eli5)):\n",
    "    passages.append(eli5['passages'][i][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load encoders \n",
    "bi_encoder = SentenceTransformer('msmarco-bert-base-dot-v5')\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2',default_activation_function=nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus embeddings \n",
    "with open('/ssd003/projects/aieng/conversational_ai/demo/data/Eli5/biencoder_embeddings/msmarco-bert-base-dot-v5.pickle', 'rb') as pkl:\n",
    "    corpus_embeddings = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_rank(query, context_size = 3, printout=True):\n",
    "    top_k=50\n",
    "    # ------ PASSAGE RETRIEVAL ------\n",
    "    start_time = time.time()\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    hits = sentenceutils.semantic_search(question_embedding, corpus_embeddings, top_k=top_k, score_function=sentenceutils.dot_score)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if printout: print(\"Input question:\", query)\n",
    "    if printout: print(\"\\n-------------------------\\n\")\n",
    "    if printout: print(\"Top 10 passages (after {:.3f} seconds):\".format(end_time - start_time))\n",
    "    \n",
    "    for hit in hits:\n",
    "        if printout: print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']]))\n",
    "        hit['passage'] = passages[hit['corpus_id']]\n",
    "    \n",
    "    # ------ RE-RANKER -----\n",
    "    # score passages\n",
    "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "    \n",
    "    # sort results\n",
    "    for i in range(len(cross_scores)):\n",
    "        hits[i]['cross-score'] = cross_scores[i]\n",
    "\n",
    "    if printout: print(\"\\n-------------------------\\n\")\n",
    "    if printout: print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    \n",
    "    for hit in hits[0:context_size]:\n",
    "        if printout: print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "        hit['context'] = passages[hit['corpus_id']]\n",
    "    return hits[0:context_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Generation using GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerGeneration:\n",
    "    \"\"\"\n",
    "    Answer Generation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.args = Namespace(\n",
    "            # fill in where you have stored the checkpoint information and file\n",
    "            model_checkpoint_dir=PATH+ \"/ssd003/projects/aieng/conversational_ai/demo/checkpoints/marco_gpt2medium/\",\n",
    "            model_checkpoint_file=PATH+ \"/ssd003/projects/aieng/conversational_ai/demo/checkpoints/marco_gpt2medium/checkpoint_epoch5_step587435.pth\",\n",
    "            no_sample=True,\n",
    "            max_length=100,\n",
    "            min_length=1,\n",
    "            seed=39,\n",
    "            temperature=0.7,\n",
    "            top_k=100,\n",
    "            top_p=0.,  # I recommend setting this to 0 so its more likely to not say \"I don't know\"\n",
    "            device=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "            force_answer=False  # discard any \"I don't know\"s and take the next best prediction\n",
    "        )\n",
    "        \n",
    "        # Initializing GPT2 Tokenizer\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.args.model_checkpoint_dir)\n",
    "\n",
    "        # Initializing pretrained model\n",
    "        config = GPT2Config.from_json_file(self.args.model_checkpoint_dir + 'config.json')\n",
    "        state_dict = torch.load(self.args.model_checkpoint_file)\n",
    "        if 'model' in state_dict:\n",
    "            state_dict = state_dict['model']\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(self.args.model_checkpoint_file, config=config,\n",
    "                                                     state_dict=state_dict)\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # add our special tokens to the model\n",
    "        add_special_tokens(self.model, self.tokenizer)\n",
    "    \n",
    "    def generate_answer(self, context, question, printout=True):\n",
    "        if not isinstance(context, list):\n",
    "            context = [context]\n",
    "        query = [self.tokenizer.encode('<speaker1>' + question)]\n",
    "        with torch.no_grad():\n",
    "            out_ids = sample_sequence(context, query, self.tokenizer, self.model, self.args)\n",
    "        response = self.tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "        if printout:\n",
    "            print(\"Answer:\", response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MSMarco Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /ssd003/projects/aieng/conversational_ai/demo/checkpoints/marco_gpt2medium/checkpoint_epoch5_step587435.pth were not used when initializing GPT2LMHeadModel: ['multiple_choice_head.summary.bias', 'multiple_choice_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Models\")\n",
    "binaryclass = BinaryClassifier()\n",
    "dialogue = DialogueGeneration()\n",
    "print(\"Loading MSMarco Model\")\n",
    "genanswers = AnswerGeneration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Categories\n",
    "\n",
    "1-Biology: 7829 \n",
    "\n",
    "2-Chemistry: 1662\n",
    "\n",
    "3-Technology: 3751\n",
    "\n",
    "4-Economics: 1644\n",
    "\n",
    "5-Physics: 2496\n",
    "\n",
    "6-Mathematics: 431\n",
    "\n",
    "7-Psychology: 79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Stats\n",
    "Train Size:  12524\n",
    "\n",
    "Validation Size:  2684\n",
    "\n",
    "Test Size:  2684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Question\n",
    "\n",
    "Q: What affects continental drift?\n",
    "\n",
    "Q: Following the passing of the Thirteenth Amendment, were there any cases of slave-owners attempting to continue the practice illegally?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSMarco Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Hi\n",
      "Input: Hi\n",
      "The class is: general with 100.00% confident\n",
      "DialoGPT Response: Hi! :D\n",
      "\n",
      "------------------------------\n",
      ">>>how are you?\n",
      "Input: how are you?\n",
      "The class is: general with 100.00% confident\n",
      "DialoGPT Response: I'm good, how are you?\n",
      "\n",
      "------------------------------\n",
      ">>>I have a question\n",
      "Input: I have a question\n",
      "The class is: general with 100.00% confident\n",
      "DialoGPT Response: What is it?\n",
      "\n",
      "------------------------------\n",
      ">>>What affects continental drift?\n",
      "Input: What affects continental drift?\n",
      "The class is: request with 100.00% confident\n",
      "Passage: The theory of plate tectonics demonstrates that the continents of the Earth are moving across the surface at the rate of a few centimeters per year. This is expected to continue, causing the plates to relocate and collide. Continental drift is facilitated by two factors: the energy generation within the planet and the presence of a hydrosphere. With the loss of either of these, continental drift will come to a halt. The production of heat through radiogenic processes is sufficient to maintain mantle convection and plate subduction for at least the next 1.1 billion years.\n",
      "The theory of plate tectonics demonstrates that the continents of the Earth are moving across the surface at the rate of a few centimeters per year. This is expected to continue, causing the plates to relocate and collide. Continental drift is facilitated by two factors: the energy generation within the planet and the presence of a hydrosphere. With the loss of either of these, continental drift will come to a halt. The production of heat through radiogenic processes is sufficient to maintain mantle convection and plate subduction for at least the next 1.1 billion years.\n",
      "Continental drift is the theory that the Earth's continents have moved over geologic time relative to each other, thus appearing to have \"drifted\" across the ocean bed. The speculation that continents might have 'drifted' was first put forward by Abraham Ortelius in 1596. The concept was independently and more fully developed by Alfred Wegener in 1912, but his theory was rejected by many for lack of any motive mechanism. Arthur Holmes later proposed mantle convection for that mechanism. The idea of continental drift has since been subsumed by the theory of plate tectonics, which explains that the continents move by riding on plates of the Earth's lithosphere.\n",
      "\n",
      "First Passage Score: 167.38104248046875\n",
      "--------------------------\n",
      "Answer: Continental drift will come to a halt.\n",
      "\n",
      "------------------------------\n",
      ">>>Following the passing of the Thirteenth Amendment, were there any cases of slave-owners attempting to continue the practice illegally?\n",
      "Input: Following the passing of the Thirteenth Amendment, were there any cases of slave-owners attempting to continue the practice illegally?\n",
      "The class is: request with 100.00% confident\n",
      "Passage: The Thirteenth Amendment to the United States Constitution abolished slavery and involuntary servitude, except as punishment for a crime. It was passed by the U.S. Senate on April 8, 1864, and, after one unsuccessful vote and extensive legislative maneuvering by the Lincoln administration, the House followed suit on January 31, 1865. The measure was swiftly ratified by all but three Union states (the exceptions were Delaware, New Jersey, and Kentucky), and by a sufficient number of border and \"reconstructed\" Southern states, to be ratified by December 6, 1865. On December 18, 1865, Secretary of State William H. Seward proclaimed it to have been incorporated into the federal Constitution. It became part of the Constitution 61 years after the Twelfth Amendment, the longest interval between constitutional amendments to date.\n",
      "The Thirteenth Amendment to the United States Constitution abolished slavery and involuntary servitude, except as punishment for a crime. It was passed by the U.S. Senate on April 8, 1864, and, after one unsuccessful vote and extensive legislative maneuvering by the Lincoln administration, the House followed suit on January 31, 1865. The measure was swiftly ratified by all but three Union states (the exceptions were Delaware, New Jersey, and Kentucky), and by a sufficient number of border and \"reconstructed\" Southern states, to be ratified by December 6, 1865. On December 18, 1865, Secretary of State William H. Seward proclaimed it to have been incorporated into the federal Constitution. It became part of the Constitution 61 years after the Twelfth Amendment, the longest interval between constitutional amendments to date.\n",
      "The Thirteenth Amendment to the United States Constitution abolished slavery and involuntary servitude, except as punishment for a crime. It was passed by the U.S. Senate on April 8, 1864, and, after one unsuccessful vote and extensive legislative maneuvering by the Lincoln administration, the House followed suit on January 31, 1865. The measure was swiftly ratified by all but three Union states (the exceptions were Delaware, New Jersey, and Kentucky), and by a sufficient number of border and \"reconstructed\" Southern states, to be ratified by December 6, 1865. On December 18, 1865, Secretary of State William H. Seward proclaimed it to have been incorporated into the federal Constitution. It became part of the Constitution 61 years after the Twelfth Amendment, the longest interval between constitutional amendments to date.\n",
      "\n",
      "First Passage Score: 174.36203002929688\n",
      "--------------------------\n",
      "Answer: Yes\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     input_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_query)\n\u001b[1;32m      5\u001b[0m     query_type, type_confidence \u001b[38;5;241m=\u001b[39m binaryclass\u001b[38;5;241m.\u001b[39mclassify(input_query)\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/ipykernel/kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1074\u001b[0m     )\n\u001b[0;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/ipykernel/kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    input_query = input('>>>')\n",
    "    print(\"Input:\", input_query)\n",
    "    \n",
    "    query_type, type_confidence = binaryclass.classify(input_query)\n",
    "    if query_type == 'general':\n",
    "        reply = dialogue.generate(input_query)\n",
    "    else:\n",
    "        context_size = 3\n",
    "        contexts = search_and_rank(input_query, context_size, printout=False)\n",
    "        context = \"\".join([contexts[i]['passage'] for i in range(context_size)])\n",
    "        print(\"Passage:\", context)\n",
    "        print(\"First Passage Score:\", contexts[0]['score'])\n",
    "        print('--------------------------')\n",
    "        reply = genanswers.generate_answer(context, input_query)\n",
    "    print('\\n------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks for your attention.\n",
    "## Any Questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
