{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/shirleyw/venvs/convai_env/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "import logging\n",
    "from pprint import pformat\n",
    "\n",
    "logger = logging.getLogger(\"FinetuningGPT2\")\n",
    "\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens and a function to add them\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ['<speaker1>', '<speaker2>', '<eou>']}\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n",
    "\n",
    "def add_special_tokens(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Add special tokens to the tokenizer and the model if they have not already been added.\n",
    "    \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "    if num_added_tokens > 0 and model is not None:\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "add_special_tokens(None, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/squad-train.json') as json_file:\n",
    "    train_stuff = json.load(json_file)\n",
    "    \n",
    "with open('data/squad-valid.json') as json_file:\n",
    "    valid_stuff = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distractor(qas, index):\n",
    "    \"\"\"\n",
    "    Get distractor answer from qas where its not at index.\n",
    "    \"\"\"\n",
    "    has_answers = len([x for x in qas if not x['is_impossible']])\n",
    "    if has_answers < 3:\n",
    "        return \"I think it's France.\"\n",
    "    new_idx = random.randint(0, len(qas) - 1)\n",
    "    if new_idx != index and not qas[new_idx]['is_impossible']:\n",
    "        return qas[new_idx]['answers'][0]['text']\n",
    "    else:\n",
    "        return get_distractor(qas, index)\n",
    "\n",
    "def create_qa_dataset(data):\n",
    "    new_data = []\n",
    "    for idx in tqdm(range(len(data))):\n",
    "        group = data[idx]\n",
    "        topic = group['title']\n",
    "        for p in group['paragraphs']:\n",
    "            data_chunk = {}\n",
    "            context = p['context']\n",
    "            data_chunk['context'] = context\n",
    "            data_chunk['topic'] = topic\n",
    "            data_chunk['utterances'] = []\n",
    "            for i in range(len(p['qas'])):\n",
    "                qa = p['qas'][i]\n",
    "                question = qa['question']\n",
    "                if qa['is_impossible']:\n",
    "                    answer = \"I'm sorry, I don't know.\"\n",
    "                    candidates = [get_distractor(p['qas'], i), get_distractor(p['qas'], i), answer]\n",
    "                else:\n",
    "                    answer = qa['answers'][0]['text']\n",
    "                    candidates = [\"I'm sorry, I don't know.\", get_distractor(p['qas'], i), answer]\n",
    "                data_chunk['utterances'].append({\n",
    "                    'history': [question],\n",
    "                    'candidates':candidates,\n",
    "                })\n",
    "            new_data.append(data_chunk)\n",
    "    return new_data\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = create_qa_dataset(train_stuff['data'])\n",
    "with open('data/squad_train.json', 'w') as json_file:\n",
    "    json.dump(squad_train, json_file, indent=2)\n",
    "    \n",
    "squad_valid = create_qa_dataset(valid_stuff['data'])\n",
    "with open('data/squad_valid.json', 'w') as json_file:\n",
    "    json.dump(squad_train, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_dataset(tokenizer, dataset_path):\n",
    "    \"\"\"\n",
    "    Get tokenized dataset.\n",
    "    \"\"\"\n",
    "    if dataset_path.endswith('tokenized.json'):\n",
    "        print(\"Loading tokenized dataset from \" + dataset_path)\n",
    "        tokenize = False\n",
    "    else:\n",
    "        if os.path.isfile(dataset_path[:-5] + '_tokenized.json'):\n",
    "            print(\"Detected existing tokenized file.\")\n",
    "            dataset_path = dataset_path[:-5] + '_tokenized.json'\n",
    "            print(\"Loading dataset from \" + dataset_path)\n",
    "            tokenize=False\n",
    "        else:\n",
    "            print(\"Loading dataset from \" + dataset_path)\n",
    "            tokenize = True\n",
    "    \n",
    "    with open(dataset_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    if tokenize:\n",
    "        print(\"Tokenizing the dataset\")\n",
    "        def tokenize(obj):\n",
    "            if isinstance(obj, str):\n",
    "                return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "            if isinstance(obj, dict):\n",
    "                return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "            return list(tokenize(o) for o in obj)\n",
    "\n",
    "        dataset = tokenize(dataset)\n",
    "        \n",
    "        new_name = dataset_path[:-5] + '_tokenized.json'\n",
    "        print(\"Saving dataset to \" + new_name)\n",
    "        with open(new_name, 'w') as outfile:\n",
    "            json.dump(dataset, outfile, indent=2)\n",
    "\n",
    "    if split:\n",
    "        print(\"Fetched \" + split + \" dataset\")\n",
    "        return dataset[split]\n",
    "    else:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = get_tokenized_dataset('data/squad_train.json')\n",
    "squad_valid = get_tokenized_dataset('data/squad_valid.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
