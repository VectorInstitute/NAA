{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2900df6f",
      "metadata": {
        "id": "2900df6f"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, evaluation, InputExample, datasets\n",
        "\n",
        "class CERerankingEvaluator:\n",
        "    \"\"\"\n",
        "    This class is a modified version of the CERerankingEvaluator from SentenceTransformers, to include more evaluation metrics. \n",
        "    It evaluates a cross-encoder model to re-rank passages. The test data is a dictionary and each key has a sub-dictionary of the form: \n",
        "    {'query': '', 'positive': [], 'negative': []}. Query is the search query,\n",
        "     positive is a list of positive (relevant) documents, negative is a list of negative (irrelevant) documents.\n",
        "    \"\"\"\n",
        "    def __init__(self, samples, mrr_at_k: int = 10, name: str = '', write_csv: bool = True): \n",
        "        self.samples = samples\n",
        "        self.name = name\n",
        "        self.mrr_at_k = mrr_at_k\n",
        "\n",
        "        if isinstance(self.samples, dict):\n",
        "            self.samples = list(self.samples.values())\n",
        "\n",
        "        self.csv_file = \"CERerankingEvaluator\" + (\"_\" + name if name else '') + \"_results.csv\"\n",
        "        self.csv_headers = [\"epoch\", \"steps\", \"MRR@{}\".format(mrr_at_k), \"MAP\"] \n",
        "        self.write_csv = write_csv\n",
        "\n",
        "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
        "        if epoch != -1:\n",
        "            if steps == -1:\n",
        "                out_txt = \" after epoch {}:\".format(epoch)\n",
        "            else:\n",
        "                out_txt = \" in epoch {} after {} steps:\".format(epoch, steps)\n",
        "        else:\n",
        "            out_txt = \":\"\n",
        "\n",
        "        all_mrr_scores = []\n",
        "        all_ap_scores = []\n",
        "        num_queries = 0\n",
        "        num_positives = []\n",
        "        num_negatives = []\n",
        "        for instance in self.samples:\n",
        "            query = instance['query']\n",
        "            positive = list(instance['positive'])\n",
        "            negative = list(instance['negative'])\n",
        "            docs = positive + negative\n",
        "            is_relevant = [True]*len(positive) + [False]*len(negative)\n",
        "\n",
        "            if len(positive) == 0 or len(negative) == 0:\n",
        "                continue\n",
        "\n",
        "            num_queries += 1\n",
        "            num_positives.append(len(positive))\n",
        "            num_negatives.append(len(negative))\n",
        "\n",
        "            model_input = [[query, doc] for doc in docs]\n",
        "            pred_scores = model.predict(model_input, convert_to_numpy=True, show_progress_bar=False)\n",
        "            pred_scores_argsort = np.argsort(-pred_scores)  #Sort in decreasing order\n",
        "            \n",
        "            # compute MRR score\n",
        "            mrr_score = 0\n",
        "            for rank, index in enumerate(pred_scores_argsort[0:self.mrr_at_k]):\n",
        "                if is_relevant[index]:\n",
        "                    mrr_score = 1 / (rank+1)\n",
        "                    break\n",
        "\n",
        "            all_mrr_scores.append(mrr_score)\n",
        "            \n",
        "            # compute AP\n",
        "            all_ap_scores.append(average_precision_score(is_relevant, pred_scores.tolist()))\n",
        "\n",
        "        mean_mrr = np.mean(all_mrr_scores)\n",
        "        mean_ap = np.mean(all_ap_scores)\n",
        "\n",
        "        if output_path is not None and self.write_csv:\n",
        "            csv_path = os.path.join(output_path, self.csv_file)\n",
        "            output_file_exists = os.path.isfile(csv_path)\n",
        "            with open(csv_path, mode=\"a\" if output_file_exists else 'w', encoding=\"utf-8\") as f:\n",
        "                writer = csv.writer(f)\n",
        "                if not output_file_exists:\n",
        "                    writer.writerow(self.csv_headers)\n",
        "\n",
        "                writer.writerow([epoch, steps, mean_mrr, mean_ap])\n",
        "\n",
        "        return {'mrr': mean_mrr, 'map': mean_ap}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9025fc0",
      "metadata": {
        "id": "f9025fc0"
      },
      "outputs": [],
      "source": [
        "# change depending on dataset\n",
        "data_folder = usr_path+ '/cross-encoder/eli5/splits/'\n",
        "output_folder = usr_path+ '/cross-encoder/eli5/results/' # to store results csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e1792e",
      "metadata": {
        "id": "b6e1792e"
      },
      "outputs": [],
      "source": [
        "test_samples = pd.read_csv(data_folder + 'test_samples.csv', converters={'positive': pd.eval, 'negative': pd.eval})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uA397arzxPib",
      "metadata": {
        "id": "uA397arzxPib"
      },
      "outputs": [],
      "source": [
        "test_samples['negative'] = test_samples['negative'].apply(set)\n",
        "test_samples['positive'] = test_samples['positive'].apply(set)\n",
        "test_samples = test_samples.to_dict('index') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rOrczOPUxRQn",
      "metadata": {
        "id": "rOrczOPUxRQn"
      },
      "outputs": [],
      "source": [
        "evaluator = CERerankingEvaluator(test_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UZ-f9mDixS30",
      "metadata": {
        "id": "UZ-f9mDixS30"
      },
      "outputs": [],
      "source": [
        "# replace with path to model \n",
        "cross_encoder = CrossEncoder('./ms-marco-MiniLM-L-6-v2_eli5/',num_labels=1, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lfCddQKmxU_d",
      "metadata": {
        "id": "lfCddQKmxU_d"
      },
      "outputs": [],
      "source": [
        "evaluator(cross_encoder, output_path=output_folder)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "reranking_evaluator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
