{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f8418b30",
      "metadata": {
        "id": "f8418b30"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "import pandas as pd \n",
        "import torch \n",
        "from torch import nn\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e57de711",
      "metadata": {
        "id": "e57de711"
      },
      "source": [
        "### Format Eli5 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2956d031",
      "metadata": {
        "id": "2956d031"
      },
      "outputs": [],
      "source": [
        "# load data from hugging face\n",
        "# can be loaded & formatted in collab if 'file not found' error appears \n",
        "dataset = load_dataset(\"vblagoje/lfqa_support_docs\", split='train') \n",
        "eli5 = pd.DataFrame(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a577c8",
      "metadata": {
        "id": "26a577c8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# delete 'meta' column \n",
        "eli5 = eli5.drop('meta', 1)\n",
        "\n",
        "# create answer column \n",
        "answers = []\n",
        "for i in range(0,len(eli5)):\n",
        "    answers.append(eli5['output'][i][:-1])\n",
        "\n",
        "for i in range(0,len(answers)):\n",
        "    for j in range(0,len(answers[i])):\n",
        "        answers[i][j] = answers[i][j]['answer']\n",
        "\n",
        "# add answer column to df\n",
        "eli5['answer'] = answers\n",
        "\n",
        "# extract related wiki passages from outputs column \n",
        "outputs = [] \n",
        "for i in range(0,len(eli5)):\n",
        "    outputs.append(eli5['output'][i][-1]['provenance']) # reduce dimension of nested dicts by removing answer keys\n",
        "\n",
        "eli5['passages'] = outputs\n",
        "eli5 = eli5.drop('output',1) # drop original outputs column "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f4c9b66",
      "metadata": {
        "id": "8f4c9b66"
      },
      "source": [
        "### Re-Rank Passages & Answers \n",
        "The original dataset has 7 wikipedia passages per query, with no indication of which passages are most relevant. Since the passage retrieval input requires 1 passage per query, a re-ranker cross-encoder was used to rank the 7 passages. The top (most relevant) passage is then selected for the input pairs. An additional field was added to include the top passage from each set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "187361ba",
      "metadata": {
        "id": "187361ba"
      },
      "outputs": [],
      "source": [
        "queries = list(eli5['input'])\n",
        "passage_list = list(eli5['passages'])\n",
        "answers = list(eli5['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc16db22",
      "metadata": {
        "id": "cc16db22"
      },
      "outputs": [],
      "source": [
        "# load pre-trained cross-encoder from sentence transformers library \n",
        "cross_encoder = CrossEncoder('/contextretrieval/cross-encoder/ms-marco-MiniLM-L-6-v2',default_activation_function=nn.Sigmoid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a3c11f",
      "metadata": {
        "id": "28a3c11f"
      },
      "outputs": [],
      "source": [
        "def re_rank(list_to_rank):\n",
        "    ranked_list = []\n",
        "    \n",
        "    for i in range(len(list_to_rank)):\n",
        "        cross_inp = []\n",
        "        for j in range(0,len(list_to_rank[i])):\n",
        "            cross_inp.append([queries[i], list_to_rank[i][j]['text']])\n",
        "\n",
        "        cross_scores = cross_encoder.predict(cross_inp)  \n",
        "        for j in range(len(cross_scores)):\n",
        "                list_to_rank[i][j]['cross-score'] = cross_scores[j]\n",
        "\n",
        "        hits = sorted(list_to_rank[i], key=lambda x: x['cross-score'], reverse=True)\n",
        "        ranked_list.append(hits)\n",
        "    return ranked_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d777711",
      "metadata": {
        "id": "3d777711"
      },
      "outputs": [],
      "source": [
        "# answers into dict format \n",
        "answers_list = []\n",
        "for i in range(len(answers)):    \n",
        "    dicts = []\n",
        "    for j in range(len(answers[i])):\n",
        "        dicts.append({'text' : answers[i][j]})\n",
        "    answers_list.append(dicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20d5c338",
      "metadata": {
        "id": "20d5c338"
      },
      "outputs": [],
      "source": [
        "ranked_passages = re_rank(passage_list)\n",
        "ranked_answers = re_rank(answers_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "654ae13d",
      "metadata": {
        "id": "654ae13d"
      },
      "outputs": [],
      "source": [
        "eli5['answer'] = ranked_answers\n",
        "eli5['passages'] = ranked_passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d460476e",
      "metadata": {
        "id": "d460476e"
      },
      "outputs": [],
      "source": [
        "# add column for top passages \n",
        "top_passages = []\n",
        "for i in range(0,len(eli5)):\n",
        "    passages.append(eli5['passages'][i][0]['text'])\n",
        "    \n",
        "eli5['passages_text'] = top_passages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ed3d7d0",
      "metadata": {
        "id": "7ed3d7d0"
      },
      "source": [
        "### Save Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb213fe4",
      "metadata": {
        "id": "fb213fe4"
      },
      "outputs": [],
      "source": [
        "eli5_reranked = eli5.to_json(orient='records')\n",
        "\n",
        "output_path = '/data/Eli5/Eli5_reranked/'\n",
        "with open(output_path + 'eli5_reranked.json', 'w') as fp:\n",
        "    json.dump(eli5_reranked, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2097fa",
      "metadata": {
        "id": "ba2097fa"
      },
      "source": [
        "## Merge with Categories Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "290dc271",
      "metadata": {
        "id": "290dc271"
      },
      "source": [
        "The version of Eli5 that contains categories (used for response generation) is much smaller than the Eli5 train dataset above. However, they share the same query IDs so the overlapping records can be merged to form a new dataset that includes the categories "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd00f0c",
      "metadata": {
        "id": "fcd00f0c"
      },
      "outputs": [],
      "source": [
        "# load from huggingface\n",
        "dataset = load_dataset(\"eli5_category\", split='train')\n",
        "eli5_categories = pd.DataFrame(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3bb409",
      "metadata": {
        "id": "1a3bb409"
      },
      "outputs": [],
      "source": [
        "eli5_merged = pd.merge(eli5, eli5_categories.rename(columns={'q_id':'id'}), on='id',  how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5155ae9",
      "metadata": {
        "id": "e5155ae9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "eli5_merged= eli5_merged.dropna()\n",
        "eli5_merged=eli5_merged.reset_index(drop=True)\n",
        "eli5_merged = eli5_merged.rename(columns={'answer': 'answers_ranked'}) \n",
        "eli5_merged = eli5_merged.rename(columns={'passages': 'passages_ranked'}) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92e15c3a",
      "metadata": {
        "id": "92e15c3a"
      },
      "source": [
        "### Save Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57dde5ad",
      "metadata": {
        "id": "57dde5ad"
      },
      "outputs": [],
      "source": [
        "eli5_categories_reranked = eli5_merged.to_json(orient=\"records\")\n",
        "with open(output_path + 'eli5_categories_reranked.json', 'w') as fp:\n",
        "    json.dump(eli5_categories_reranked, fp)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "eli5_formatting.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
