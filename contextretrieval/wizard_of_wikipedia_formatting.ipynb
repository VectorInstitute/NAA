{"cells":[{"cell_type":"markdown","id":"nudUaV6Dy8I2","metadata":{"id":"nudUaV6Dy8I2"},"source":["The Wizard of Wikipedia dataset contains user inputs, answers, and a relevant Wikipedia ID per input. The ID references the relevant passage in a wikipedia knowledge base. The following script maps the IDs to the passages and appends them to the main dataset. It is further formatted to the proper format needed for both the bi-encoder and cross-encoder models."]},{"cell_type":"code","execution_count":3,"id":"193fd30f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23867,"status":"ok","timestamp":1655323945377,"user":{"displayName":"Elham Dolatabadi","userId":"12355888834359074190"},"user_tz":240},"id":"193fd30f","outputId":"45eab763-73fb-4aa0-c387-fb55f4b12098"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n","\u001b[K     |████████████████████████████████| 362 kB 4.9 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 45.6 MB/s \n","\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.7.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 44.8 MB/s \n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 63.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.5.18.1)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 64.3 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 66.4 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.0 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 61.9 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.3.2 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}],"source":["!pip install transformers\n","!pip install datasets\n","from datasets import load_dataset\n","import json \n","import pandas as pd\n","import random "]},{"cell_type":"code","execution_count":null,"id":"9_PL9hY6Ag99","metadata":{"id":"9_PL9hY6Ag99"},"outputs":[],"source":["dataset = load_dataset(\"namespace/your_dataset_name\", data_files=data_files)\n"]},{"cell_type":"code","execution_count":null,"id":"074e2f71","metadata":{"id":"074e2f71"},"outputs":[],"source":["# load wizard of wikipedia from hugging face\n","wow_train = load_dataset(\"kilt_tasks\", \"wow\", split='train')\n","wow_val = load_dataset(\"kilt_tasks\", \"wow\", split='validation')"]},{"cell_type":"code","execution_count":null,"id":"d4c4163e","metadata":{"id":"d4c4163e"},"outputs":[],"source":["# combine both datasets (splits generated later)\n","wow = pd.concat([pd.DataFrame(wow_train), pd.DataFrame(wow_val)], axis=0)\n","wow = wow.sample(frac=1).reset_index(drop=True) #shuffle dataset"]},{"cell_type":"code","execution_count":null,"id":"7dbae7cf","metadata":{"id":"7dbae7cf"},"outputs":[],"source":["# find all unique wikipedia ids to extract from knowledge base\n","# find indices of inputs with no passage\n","needed_ids = []\n","irrelevant_inputs = []\n","for i in range(0, len(wow)):\n","    if wow['output'][i][0]['provenance'] != []:\n","        needed_ids.append(wow['output'][i][0]['provenance'][0]['wikipedia_id'])\n","    else:\n","        irrelevant_inputs.append(i)\n","\n","# filter out duplicate IDs \n","needed_ids = list(set(needed_ids))"]},{"cell_type":"code","execution_count":null,"id":"e1e8973d","metadata":{"id":"e1e8973d"},"outputs":[],"source":["# drop recrods with no passage \n","no_passage_idx = wow.index[irrelevant_inputs]\n","wow = wow.drop(no_passage_idx).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"id":"ae2ed98d","metadata":{"id":"ae2ed98d"},"outputs":[],"source":["# filter knowledge base to only keep needed IDs\n","wiki_knowledge = wiki_knowledge.filter(lambda x: x['wikipedia_id'] in needed_ids)"]},{"cell_type":"code","execution_count":null,"id":"1d9e88bc","metadata":{"id":"1d9e88bc"},"outputs":[],"source":["# collect the subsections containing the passages \n","# the subsection of each passage is specified in the data\n","\n","needed_sections = []\n","\n","for i in range(0, len(wow)):\n","    if wow['output'][i][0]['provenance'] != []:\n","        needed_sections.append([wow['output'][i][0]['provenance'][0]['wikipedia_id'], wow['output'][i][0]['provenance'][0]['section']])"]},{"cell_type":"code","execution_count":null,"id":"51bbc51a","metadata":{"id":"51bbc51a"},"outputs":[],"source":["# using the ID and needed subsection, collect all passages from knowledge base\n","\n","passages = []\n","\n","for i in range(0, len(needed_sections)):\n","    row = wiki_knowledge.loc[wiki_knowledge['wikipedia_id'] == needed_sections[i][0]]\n","    section_list = row['text'].values[0]['paragraph']\n","    section = needed_sections[i][1]\n","    \n","    if section == 'Section::::Abstract.':\n","        passage = section_list[1]\n","    else:\n","        passage = section_list[section_list.index(section) + 1]\n","    \n","    passages.append(passage)"]},{"cell_type":"code","execution_count":null,"id":"65e70c94","metadata":{"id":"65e70c94"},"outputs":[],"source":["# extract answers from nested output\n","answers = []\n","for i in range(0, len(wow)):\n","    answers.append(wow['output'][i][0]['answer'])"]},{"cell_type":"code","execution_count":null,"id":"9ec60ef6","metadata":{"id":"9ec60ef6"},"outputs":[],"source":["# add passage and answer columns to dataset\n","wow['answer'] = answers\n","wow['passages_text'] = passages"]},{"cell_type":"code","execution_count":null,"id":"9cd0ff7c","metadata":{"id":"9cd0ff7c"},"outputs":[],"source":["# save dataset \n","wow = wow.to_json()\n","\n","output_folder = usr_path+ '/data/Wizard_of_Wikipedia/'\n","\n","with open(output_folder+'wizard_of_wikipedia.json', 'w') as fp:\n","    json.dump(wow, fp)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"wizard_of_wikipedia_formatting.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":5}
