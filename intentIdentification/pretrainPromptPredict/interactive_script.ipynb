{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba21205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 202 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (28997, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "cls.predictions.transform.dense.weight                    (768, 768)\n",
      "cls.predictions.transform.dense.bias                          (768,)\n",
      "cls.predictions.transform.LayerNorm.weight                    (768,)\n",
      "cls.predictions.transform.LayerNorm.bias                      (768,)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer, BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def print_model_info(model):\n",
    "    # Get all of the model's parameters as a list of tuples.\n",
    "    params = list(model.named_parameters())\n",
    "\n",
    "    print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "    for p in params[-4:]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "# def tokenize_sentences(sentences):\n",
    "#     # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "\n",
    "#     # For every sentence...\n",
    "#     for sent in sentences:\n",
    "#         # encode_plus will:\n",
    "#         #   (1) Tokenize the sentence.\n",
    "#         #   (2) Prepend the [CLS] token to the start.\n",
    "#         #   (3) Append the [SEP] token to the end.\n",
    "#         #   (4) Map tokens to their IDs.\n",
    "#         #   (5) Pad or truncate the sentence to max_length\n",
    "#         #   (6) Create attention masks for [PAD] tokens.\n",
    "#         encoded_dict = tokenizer.encode_plus(\n",
    "#                             sent,                      # Sentence to encode.\n",
    "#                             add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "#                             max_length = 320,           # Pad & truncate all sentences.\n",
    "#                             padding='max_length',\n",
    "#                             return_attention_mask = True,   # Construct attn. masks.\n",
    "#                             return_tensors = 'pt',     # Return pytorch tensors.\n",
    "#                     )\n",
    "        \n",
    "#         # Add the encoded sentence to the list.    \n",
    "#         input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "#         # And its attention mask (simply differentiates padding from non-padding).\n",
    "#         attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "#     # Convert the lists into tensors.\n",
    "#     input_ids = torch.cat(input_ids, dim=0)\n",
    "#     attention_masks = torch.cat(attention_masks, dim=0)\n",
    "#     return input_ids,attention_masks        \n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "special_tokens_dict = {'additional_special_tokens': ['[BLK]']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "class BertPred(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "        self.bert.resize_token_embeddings(len(tokenizer))\n",
    "        print_model_info(self.bert)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        loss = 0\n",
    "#         if labels is not None:\n",
    "#             loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "\n",
    "new_model = BertPred()\n",
    "new_model.load_state_dict(torch.load('saved.bin'))\n",
    "new_model.eval()\n",
    "\n",
    "chk_path = \"/h/elau/Conv_BERT/BERT_MLM_OOD/checkpoints_OOD/best-checkpoints.ckpt\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# checkpoint = torch.load('saved.bin', map_location=torch.device('cpu'))\n",
    "\n",
    "# model = BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# print(model.state_dict().bert.keys())\n",
    "\n",
    "#load specified model state\n",
    "# print(checkpoint.keys())\n",
    "# # print(checkpoint[\"state_dict\"].keys())\n",
    "# model.load_state_dict(checkpoint)\n",
    "# model.eval()\n",
    "new_model.to(device)\n",
    "model = new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc4463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How may I help you? Question: I want to do a cash top-up\n",
      "I see. The problem is related to:  topping cash\n",
      "================================================================================\n",
      "How may I help you? Question: How long until my card is delivered?\n",
      "I see. The problem is related to:  visa card\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    string = input(\"How may I help you? Question: \")\n",
    "    #tokenize inputted sentence to be compatible with BERT inputs\n",
    "#     token_ids,attention_masks = tokenize_sentences([string + \" the reason of the question is \" + \" [BLK]\"*6])\n",
    "    #get a tensor containing probabilities of inputted sentence being irrelevant or relevant\n",
    "#     token_logits = (model(token_ids.to(device)))\n",
    "#     softmax_layer = torch.nn.Softmax()\n",
    "#     result = softmax_layer(model_outputs[0])\n",
    "#     #identify which output node has higher probability and what that probability is\n",
    "#     prediction = torch.argmax(result).item()\n",
    "#     confidence = torch.max(result).item()\n",
    "    \n",
    "#     print(LABEL_COLUMNS[prediction])\n",
    "#     print(\"{:.2f}% confident\".format(confidence*100))\n",
    "    string = string + \" the reason of the question is \" + \" [MASK]\"*6\n",
    "    \n",
    "    inputs = tokenizer.encode_plus(\n",
    "      string,\n",
    "      add_special_tokens=True,\n",
    "      max_length=300,\n",
    "      return_token_type_ids=False,\n",
    "      padding=\"max_length\",\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "    token_logits = model(inputs['input_ids'].to(device), inputs['attention_mask'].to(device))[1]\n",
    "    predicted_intent = ''\n",
    "    for ele in mask_token_index[0:2]: \n",
    "        mask_token_logits = token_logits[0][0, [ele], :]\n",
    "        top_5_tokens = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()\n",
    "        for token in top_5_tokens:\n",
    "            predicted_intent = predicted_intent + \" \" + tokenizer.decode([token])\n",
    "    print(\"I see. The problem is related to: \" + predicted_intent)\n",
    "    print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b0519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
