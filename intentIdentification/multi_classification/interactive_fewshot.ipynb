{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecaeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 77)\n",
    "\n",
    "#         checkpoint = torch.load(chk_path, map_location=torch.device('cpu'))\n",
    "        ### New layers:\n",
    "#         self.linear1 = nn.Linear(768, 77)\n",
    "        for self.param in self.bert.bert.parameters():\n",
    "    #         print(\"HERE\")\n",
    "            self.param.requires_grad = False\n",
    "        self.classifier = nn.Linear(77, 15) \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = torch.sigmoid(self.bert(input_ids, attention_mask=attention_mask)[0])\n",
    "        #     print(output)\n",
    "        output = self.classifier(output)\n",
    "#         output = torch.sigmoid(output)\n",
    "        #     print(output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f27372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import (\n",
    "    BertForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    BertModel, \n",
    "    BertConfig, \n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification\n",
    ")\n",
    "def print_model_info(model):\n",
    "    # Get all of the model's parameters as a list of tuples.\n",
    "    params = list(model.named_parameters())\n",
    "\n",
    "    print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "    for p in params[-4:]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        # encode_plus will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the [CLS] token to the start.\n",
    "        #   (3) Append the [SEP] token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to max_length\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 320,           # Pad & truncate all sentences.\n",
    "                            padding='max_length',\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    return input_ids,attention_masks        \n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "chk_path = \"/intentidentification/multiclass/checkpoints_OOS/best-checkpoint.ckpt\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "checkpoint = torch.load(chk_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#     num_labels = len(LABEL_COLUMNS),\n",
    "#     state_dict=checkpoint\n",
    "# )\n",
    "\n",
    "model = CustomBERTModel()\n",
    "\n",
    "print_model_info(model)\n",
    "\n",
    "#load specified model state\n",
    "# print(checkpoint[\"state_dict\"].get('bert.embeddings.word_embeddings.num_embeddings'))\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9650962",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = ['replacement_card_duration',\n",
    " 'expiration_date',\n",
    " 'damaged_card',\n",
    " 'improve_credit_score',\n",
    " 'report_lost_card',\n",
    " 'card_declined',\n",
    " 'credit_limit_change',\n",
    " 'apr',\n",
    " 'redeem_rewards',\n",
    " 'credit_limit',\n",
    " 'rewards_balance',\n",
    " 'application_status',\n",
    " 'credit_score',\n",
    " 'new_card',\n",
    " 'international_fees']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd947e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    string = input(\"How may I help you? Question: \")\n",
    "    #tokenize inputted sentence to be compatible with BERT inputs\n",
    "    token_ids,attention_masks = tokenize_sentences([string])\n",
    "    #get a tensor containing probabilities of inputted sentence being irrelevant or relevant\n",
    "    model_outputs = (model(token_ids.to(device), attention_mask=attention_masks.to(device)))\n",
    "#     print(model_outputs)\n",
    "    softmax_layer = torch.nn.Softmax()\n",
    "    result = softmax_layer(model_outputs[1])\n",
    "#     print(result)\n",
    "    #identify which output node has higher probability and what that probability is\n",
    "    prediction = torch.argmax(result).item()\n",
    "    confidence = torch.max(result).item()\n",
    "    print(\"I see. The problem is related to: \" + LABEL_COLUMNS[prediction].replace(\"_\", ' ') + \" with {:.2f}% confident\".format(confidence*100))\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_explainer = SequenceClassificationExplainer(model=model, tokenizer=tokenizer)\n",
    "\n",
    "while(True):\n",
    "    string = input(\"How may I help you? Question: \")\n",
    "    #tokenize inputted sentence to be compatible with BERT inputs\n",
    "    token_ids,attention_masks = tokenize_sentences([string])\n",
    "    #get a tensor containing probabilities of inputted sentence being irrelevant or relevant\n",
    "    model_outputs = (model(token_ids.to(device), token_type_ids=None, attention_mask=attention_masks.to(device)))\n",
    "    softmax_layer = torch.nn.Softmax()\n",
    "    result = softmax_layer(model_outputs[0])\n",
    "    #identify which output node has higher probability and what that probability is\n",
    "    prediction = torch.argmax(result).item()\n",
    "    confidence = torch.max(result).item()\n",
    "    print(\"I see. The problem is related to: \" + LABEL_COLUMNS[prediction].replace(\"_\", ' ') + \" with {:.2f}% confident\".format(confidence*100))\n",
    "    word_attributions = multiclass_explainer(text=string)\n",
    "    print(\"-\"*80)\n",
    "    print(word_attributions)\n",
    "    html = multiclass_explainer.visualize()\n",
    "    print(html)\n",
    "    print(\"*\"*100)\n",
    "    print(\"*\"*100)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
